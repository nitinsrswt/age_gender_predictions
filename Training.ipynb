{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d91cea",
   "metadata": {},
   "source": [
    "# Gender and Age Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f08b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Dropout, BatchNormalization, Flatten, Input\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0e05395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the path .\n",
    "datasetFolder = r\"C:\\Users\\ACER\\Documents\\Gender Detection\\DataSets\\UTKFace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8fdfa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty list.\n",
    "\n",
    "pixels = []\n",
    "age = []\n",
    "gender = []\n",
    "for img in os.listdir(datasetFolder) : # os.listdir opens the directory \"datasetFolder\"\n",
    "    # Label of each image is splitted on \"_\" and required information is stored in required variable.\n",
    "    ages = img.split(\"_\")[0] \n",
    "    genders = img.split(\"_\")[1]\n",
    "    img = cv2.imread(str(datasetFolder) + \"/\" + str(img)) # Reading each image from the path of folder provided.\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Converting the input image from BGR to RGB as computer by default sees an image in BGR.\n",
    "    \n",
    "    # Appending necessary data in respective created lists.\n",
    "    pixels.append(np.array(img))\n",
    "    age.append(np.array(ages))\n",
    "    gender.append(np.array(genders))\n",
    "\n",
    "# Converting list to array\n",
    "age = np.array(age, dtype = np.int64) \n",
    "pixels = np.array(pixels)\n",
    "gender = np.array(gender, np.uint64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4dddc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of images working upon 23708\n"
     ]
    }
   ],
   "source": [
    "# Printing the length of the pixel .\n",
    "p = len(pixels)\n",
    "print(f\"No. of images working upon {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02dbd225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the images in train and test dataset.\n",
    "x_train, x_test, y_train, y_test = train_test_split(pixels, age, random_state = 100)\n",
    "\n",
    "# Splitting the dataset in train and test dataset as gender as.\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(pixels, gender, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb2f244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17781, 200, 200, 3),\n",
       " (17781, 200, 200, 3),\n",
       " (5927, 200, 200, 3),\n",
       " (5927, 200, 200, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of the images set. Here (200, 200, 3) are height, width and channel of the images respectively.\n",
    "x_train.shape, x_train_2.shape, x_test.shape, x_test_2.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4fb1670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17781,), (17781,), (5927,), (5927,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the shape of the target variable.\n",
    "y_train.shape, y_train_2.shape, y_test.shape, y_test_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4c30c",
   "metadata": {},
   "source": [
    "###### Below cell of code is used to create layers of a convolution neural network model. The layers in a CNN model are : \n",
    "* Input Layer\n",
    "* Convolution Layer\n",
    "* ReLu Layer\n",
    "* Pooling Layer\n",
    "* Fully Connected Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ce59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayer = Input(shape = (200, 200, 3)) # From the Input Model called from keras.models. Again (200, 200, 3) are height, width and channel of the images respectively.\n",
    "convLayer1 = Conv2D(140,(3,3), activation = 'relu')(inputLayer) \n",
    "'''An activation function is basically just a simple function that transforms its inputs into outputs that have a certain range.\n",
    "Also the ReLu activation transforms the -ve vaulues into 0 and positive remains the same, hence it is known as half rectifier as\n",
    "well.'''\n",
    "convLayer2 = Conv2D(130,(3,3), activation = 'relu')(convLayer1) # Creating seccond layer of CNN.\n",
    "batch1 = BatchNormalization()(convLayer2) # Normalizing the data.\n",
    "poolLayer3 = MaxPool2D((2,2))(batch1) # Creating third, Pool Layer of the CNN.\n",
    "convLayer3 = Conv2D(120,(3,3), activation = 'relu')(poolLayer3) # Adding the third Layer.\n",
    "batch2 = BatchNormalization()(convLayer3) # Normalizing the layer.\n",
    "poolLayer4 = MaxPool2D((2,2))(batch2) #Adding fourth layer of CNN. \n",
    "flt = Flatten()(poolLayer4) # Flattening the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0108316",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_model = Dense(128,activation=\"relu\")(flt) # Here 128 is the no. of neurons connected with the flatten data layer.\n",
    "age_model = Dense(64,activation=\"relu\")(age_model) #Now as we move down, no. of neurons are reducing with previous neurons connected to them.\n",
    "age_model = Dense(32,activation=\"relu\")(age_model) \n",
    "age_model = Dense(1,activation=\"relu\")(age_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "559e688d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Softmax is a mathematical function that converts a vector of numbers into a vector of probabilities, where the probabilities \\nof each value are proportional to the relative scale of each value in the vector. Here it is used as an activation function.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_model = Dense(128,activation=\"relu\")(flt) # The same work as above with 128 neurons is done for gender predictive model.\n",
    "gender_model = Dense(80,activation=\"relu\")(gender_model)\n",
    "gender_model = Dense(64,activation=\"relu\")(gender_model)\n",
    "gender_model = Dense(32,activation=\"relu\")(gender_model)\n",
    "gender_model = Dropout(0.5)(gender_model) # Drop-out layer is added to dodge the overfitting of the model.\n",
    "'''Softmax is a mathematical function that converts a vector of numbers into a vector of probabilities, where the probabilities \n",
    "of each value are proportional to the relative scale of each value in the vector. Here it is used as an activation function.'''\n",
    "gender_model = Dense(2,activation=\"softmax\")(gender_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed82e43",
   "metadata": {},
   "source": [
    "###### Below cell of code is to make an object of the Model from keras.models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8572ffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200, 200, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 198, 198, 140 3920        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 196, 196, 130 163930      conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 196, 196, 130 520         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 98, 98, 130)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 96, 96, 120)  140520      max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 96, 96, 120)  480         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 48, 48, 120)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 276480)       0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 80)           22118480    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           5184        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           17694784    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2080        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            33          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2)            66          dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 40,132,077\n",
      "Trainable params: 40,131,577\n",
      "Non-trainable params: 500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inputLayer,outputs=[age_model,gender_model]) # Adding the input layer and the output layer in our model and making the object.\n",
    "model.compile(optimizer=\"adam\",loss=[\"mse\",\"sparse_categorical_crossentropy\"],metrics=['mae','accuracy']) \n",
    "model.summary() # To get the summary of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093f9ba",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "save = model.fit(x_train,[y_train,y_train_2], validation_data=(x_test,[y_test,y_test_2]),epochs=50)\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b70e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
